% ---------
%  Compile with "pdflatex report".
% --------
%!TEX TS-program = pdflatex
%!TEX encoding = UTF-8 Unicode

\documentclass[11pt]{article}
\usepackage{jeffe,handout}
\usepackage[utf8]{inputenc}     % Allow some non-ASCII Unicode in source

% =========================================================
%   Define common stuff for solution headers
% =========================================================
\Class{CS 460}
\Semester{Spring 2018}
\Authors{1}
\AuthorOne{Tristan Gurtler}{tgurtler}

% =========================================================
\begin{document}

% ---------------------------------------------------------
\HomeworkHeader

\section{Introduction}
\label{intro}
For my final project, I aimed broadly to investigate malevolent activity on Twitter. I had multiple ideas, though they largely fell into one of two categories: detecting illicit activity by bot accounts posing as people on Twitter, and detecting abusive activity by human individuals on Twitter. This was largely inspired by the, at the time, apparently numerous missteps made by Twitter themselves to handle the issue. As I will discuss in section \ref{twitter-efforts}, I seem to have picked a portentous time to do this, as Twitter themselves have recently made significant successes to curb such behavior. While this is good for the ecosystem of Twitter, it does have the unfortunate consequence that my work to detect malevolent behavior was made more difficult by the efforts Twitter took to punish those engaging in it.

This report will proceed as follows. In section \ref{intro}, we introduce the report. In section \ref{motivation}, we discuss what relevance bot accounts and trolls have to Twitter as a medium. In section \ref{twitter-efforts}, we discuss a timeline of efforts Twitter has made to abate such malevolent behavior on its platform, including the consequences on current research. In section \ref{bot-detect}, we discuss current efforts by academics done to try to detect bot behavior on Twitter. In section \ref{troll-detect}, we discuss efforts by various groups to try to detect specifically the incidence of ``Russian troll factory'' accounts -- accounts allegedly run by operatives of the Russian Federation for agitprop purposes. In section \ref{desired-tools}, we outline the tools suggested we would create for this project, and methodological successes and failures we had there. In section \ref{implementation-results}, we discuss the actual results we observed of the tools we did build. In section \ref{conclusion}, we conclude.

\section{Motivation}
\label{motivation}
Even from before the 2016 American Presidential Election, individuals have been drawing attention to non-human activity on Twitter and the impact that it could have. As early as 2013, folks were beginning to write out about the prevalence of bots on Twitter, although at that point they were largely seen as either designed to post spam and get users to click suspicious links or to post bizarre and humorous content \cite{bots-humor}. As 2016 drew nearer, more attention was drawn to the effect bots had on public discourse regarding the election \cite{bots-election}. Allegations that bots were manipulating the outcome of online social media polls about the candidates, or were being used to astroturf support abounded. In fact, tools used to track bots today show a particular interest in political events; among tweets including the hashtag ``\#MAGA'', allegedly around two-thirds are actually posted by bot accounts \cite{botcheck}. Twitter bots have been shown as being parts of efforts to disseminate fake news across the site leading up to the 2016 election \cite{anatomy}. Today, bots can be shown to post approximately two-thirds of all links appearing on Twitter to popular news and current event sites \cite{bots-twittersphere}. Bots have had a large presence on Twitter for several years besides merely doing things like posting gibberish, although even this is of interest in some cases. Twitter specifically has been interested in curbing the use of bots to inflate follower counts and manipulate Twitter algorithms \cite{followers-vanish}.

Meanwhile, abusive behavior on Twitter has also generated interest, whether human or not. Notably, openly identifying members of the American Nazi Party and other white supremacist groups were allowed to use the platform unperturbed until December of 2017 \cite{nazis-twitter}. Actions Twitter has taken to curb abusive behavior have proven controversial at times; most notably alleged bias against conservative users in particular. Project Veritas -- the group now infamous for their botched attempt to manipulate the Washington Post into posting untrue information about Roy Moore in an attempt to discredit other individuals who spoke out against him \cite{veritas-failure} -- in particular claimed to have proof of Twitter shadowbanning conservatives to censor political thought it opposed \cite{veritas-claim}. Other claims include that swearing at famous people, like Vice President Mike Pence, will get users' accounts locked \cite{swears}. Getting these things right are difficult, though clearly worth pursuing.

\section{Twitter's efforts at curbing abuse}
\label{twitter-efforts}
For a significant period of time, Twitter did not seem to put significant effort into curbing bot or abusive behavior. Prior to 2015 especially, Twitter had a very low track record of banning or suspending users from its platform \cite{suspensions}. 2016 marked an uptick, including notable banning of Milo Yiannopoulos for inciting abusive behavior towards actress Leslie Jones \cite{milo}. 2017, however, was when Twitter made itself most clear about its desire to clean up the platform of abusive behavior. Twitter CEO Jack Dorsey publicly promised ``a completely new approach to abuse on Twitter''; what this ended up meaning was that tweets deemed abusive would be more difficult to see, hidden behind extra layers of buttons to click \cite{abusive-twitter}. How tweets were deemed abusive was never announced though, a trend Twitter has stated was in order to avoid malevolent users attempting to game the system. Some users claimed that obscene language, like swearing at other users \cite{swears} and words generally deemed rude (e.g. ``retard'') were getting them punished, though other efforts to confirm this seemed to indicate that single acts of such behavior were insufficient to find oneself punished \cite{probably}. Twitter notably faced some severe hiccups with this technology, though, banning one Japanese man for making a death threat to a mosquito \cite{mosquito}, as well as for sexually harrassing Tony the Tiger \cite{tony}. The man banned in the Tony the Tiger case said of the situation, ``America has this back-ass-wards Calvinist streak where calling for the expulsion and genocide of non-white races is just a difference of opinion, but making a sex joke at a corporate mascot who paid money to advertise to you is cause for censure.'' His implication, it was clear, was that Twitter wasn't focusing its attention on places that actually mattered. While another rule change coming late in 2017 ended up more directly addressing this concern \cite{nazis-twitter}, Twitter had certainly not had the best track record to that point.

While they were busy attempting to put out the fire of abusive behavior, Twitter seemed not to pay much attention to bots. In early 2018 was when Twitter began to take more drastic efforts to curb bot abuse, specifically making changes to its API to attempt to make it more difficult for spam bots and bots designed to propagate information widely across multiple accounts to operate \cite{curb-bots}. The timing of this announcement, shortly after Twitter confirmed that over 50,000 bots with ties to Russia appeared to have attempted to interfere with the 2016 Election, seems to acknowledge the large problem Twitter has had with bots. This too, however, has not been an effort without issues. Very recently, Twitter had a run of users negatively impacted by an algorithm designed to detect spam tweets; specifically, users who used the word ``thanks'' or ``thank you'' in tweets found their accounts limited until they proved they were human \cite{thanks}.

While there are significant methodological issues for a student project on automatically identifying abusive behavior (that will be covered in section \ref{desired-tools}), another notable problem that I observed was that it actually was difficult as a normal user to find abusive content. While it certainly still existed, the efforts Twitter has taken to ``limit its reach'' do legitimately mean that bare effort attempts to, for example, find Nazi-affiliated accounts are next to impossible (this would seem to make sense, as if they weren't, Twitter would likely have banned them already as part of the late 2017 policy changes). This made my work significantly more difficult than I anticipated when I initially set out on this project. 

\section{Current techniques to detect bots}
\label{bot-detect}
In academic circles, between detecting bots and detecting abusive or manipulative human behavior, it appears that most work has focused on detecting bots. This seems relatively unsurprising, as unlike essentially performing some kind of ``truth detector'' like ritual on an account to determine its sincerity, bots could unwittingly leave traces of their inhumanity that could be tracked. I personally found three relatively large (numerous publications in larger conferences/journals) projects, and will discuss their main methodologies here.

\subsection{Botometer}
The most comprehensive project, by far, is Botometer \cite{botometer-main}. Mostly conducted out of Indiana University, the project aims to detect bot behavior with a claimed 1,150 features fed into a machine learning amalgamation. These features came from six different broad domains. Four of these are language-independent: User features (features revolving around a single account, like if it has a default profile image, or how frequently it posts), ``Friends'' features (features revolving around other users the account in question retweets/mentions and is retweeted/mentioned by), Network features (largely, applying PageRank algorithms to the networks of retweeting/mentioning a particular account finds itself in to determine the topology that the account usually posts to), and Timing features (features revolving around the length of the time intervals between when a user posts or retweets things). The other two domains are largely English specific: Content features (features revolving grammaticality and understandability of posts made by an account) and Sentiment features (features revolving around things like how positive or negative a tweet was). This veritable deluge of features allows Botometer to claim accuracy rates as high as 89\% (suspiciously, higher than their own inter-annotator agreement) \cite{botometer-paper}. I attempted to replicate this tool (as seen in section \ref{desired-tools}); in doing so, I noticed two particular things. First, although at \cite{botometer-main} it appears that the tool can largely be used standalone on any account, many of the features it claims to use actually track accounts over time (for example, watching to see how many follows it has over time, to try to detect if the account churns through accounts it follows quickly or grows a group of people to follow more naturally); this would indicate that either the tool has a massive database it keeps updated with information about a huge number of Twitter accounts, or that its release form features a more relaxed version with fewer features. Due to this, in my replication, I attempted to replicate the publicly available tool, instead of the original research tool (and thus had lower accuracy, around 80\%). Second, Twitter rate-limiting is particularly odious and, well, limiting -- even working with a limited pool of accounts to keep track of, it would be a wonder for their tool to keep up with as many accounts as they claim to; merely doing one iteration on every account would take (at the rates available to apps that don't pay Twitter, at least) approximately two days, at minimum, due to certain API calls it makes being particularly limited. As such, I worked with a significantly smaller, random pool of accounts, also leading to my lower accuracy.

\subsection{DeBot}
The next substantial project, out of the University of New Mexico, is DeBot \cite{debot}. Their main technique is to observe that bots at some times seem to directly copy the activity of other accounts. Two seemingly unrelated accounts will at times post the exact same concept; doing this for long enough is indicative of automated behavior. Although \cite{botometer-paper} claims that a small number of bots will behave this way, DeBot claims to have found over 730,000 distinct bots since February 2015. It remains to be seen if their technique to detect bots will withstand the API changes Twitter made in early 2018 that more directly prohibit the copying activity between multiple accounts DeBot relies on to discover bots \cite{curb-bots}.

\subsection{NYU Bot Detection}
There also appears to be a group operating out of NYU working on bot detection on Twitter \cite{nyu}. It is unclear exactly what techniques they use to detect bots; only that their focus was specifically on two periods of time relevant to Russian politics (most of 2014 and most of 2015, separately). Based on what they report, they may have used similar techniques to Botometer, as they directly claim that the single most powerful predictor for an account being a bot is if it posted via web instead of via a mobile phone. What features besides this (and retweet rate and geo-location being turned on) they actually may have used, though, are not clear. It is also notable that they confirm that bots seem to have a large hand in spreading news stories and potentially manipulating which news stories have attention drawn to them (similar to \cite{anatomy}).

\section{Current techniques to detect trolls}
\label{troll-detect}
Although Russia isn't the only country to use social media agitprop for its own devices \cite{replyall}, it certainly has the most attention in the United States. Twitter identified 2,752 accounts tied to Russia's ``Internet Research Agency'' troll farm \cite{list}, and more still appear to exist \cite{hamilton}. While we can observe their behavior (trying to blend in as American citizens, local news agencies, and accounts tied to local political groups), and watch as they do much the same as we've observed in bots (focus on distributing specific information, rather than directly inventing it) \cite{neo}, this doesn't give us many pointers on how to actually detect them. We do get some clues; posting is significantly more frequent on these types of accounts during normal business hours in Moscow instead of during times Americans might expect to be awake, but this by itself is not proof. Some groups have given advice for things to watch out for (grammatical mistakes around words like ``a'' and ``the'', which do not have equivalents in Russian, or regarding question word order, which differs between Russian and English), but these aren't sufficient either \cite{dfrlab}. Probably the best advice is just to watch out for accounts that, if you go back through their history, in particular seem to follow along with the narrative that the Kremlin is trying to establish for particular news stories. However, identifying this automatically seems very difficult to do (how can you direct a computer to identify somebody holding to a specific narrative about a news story?), and even for humans this takes an inordinate amount of time and research every time you encounter somebody you might have reason to check up on. At large, although the problem of detecting bots automatically seems to have been reasonably well-studied, detecting troll behavior automatically seems considerably more difficult.

\section{Tools described at project outset}
\label{desired-tools}
In my project proposal, I outlined four tools that I was interested in building in regards to the problems observed across this paper. I wanted to replicate and improve on current techniques of identifying bots on Twitter, to replicate and improve on current techniques of identifying trolls on Twitter, to detect brigading on Twitter, and to detect abusive behavior beyond just swearing on Twitter. In the end, I was only able to partially accomplish the first of these; I will now discuss why that is.

\subsection{Replication/Improvement of bot detection}
As discussed in section \ref{bot-detect}, technological means for detecting bots is actually fairly mature. Botometer \cite{botometer-main} in particular is very well fleshed out with relatively high success rates. I spent a good chunk of my coding time just understanding what this system actually even did to make its measurements and build its ML system. I noticed two particular things about the system, as mentioned earlier. The tool that is publicly advertised seems able to be used standalone on any account, even though many of the features it claims to use actually track accounts over time. This would indicate that either the tool has a massive database it keeps updated with information about a huge number of Twitter accounts, or that its release form features a more relaxed version with fewer features. Due to this, in my replication, I attempted to replicate the publicly available tool, instead of the original research tool. Second, Twitter rate-limiting is particularly limiting, and as such, I worked with a significantly smaller pool of accounts. Even so, collecting data took several hours. Another note I had was that some of its features depended on language specific features; this too would have required its own corpus of ``valid Twitter speech''; I largely did not replicate this, as they did not outline very clearly where they obtained such a corpus. This replication can be found in my repo.

\subsection{Replication/Improvement of troll detection}
At the outset of this project, I did not realize anywhere near how daunting a task identifying the behavior of trolls is. We could broaden this to detect ``bad faith'' actors (i.e. individuals who seem to be arguing in bad faith with another individual or otherwise intending to aggravate others), which would avoid the inherent magic of attribution that we would otherwise need to implement, but even then we need some form of gold standard from which we can decide whether somebody is acting in bad faith or not. To do this, we would need a pool of accounts to investigate and human annotators; in order to get a sufficient number to work with, we'd likely need a good number of annotators to do that work. Generating this dataset quickly goes out of scope from what an undergrad with a budget of \$0 can do to something that actually needs monetary support to be able to accomplish (since, to my knowledge, no such corpus already exists, unlike with the bot detection systems). I did consider even more significant relaxations of the question at hand (specifically, can I detect the account of a Nazi), but ran into the problem that if an account belonged obviously enough to somebody who identified as a Nazi (e.g. swastikas in their profile pic), they would have certainly already been banned. Though this doesn't mean that there are no Nazis on Twitter, finding them would then be subject to much the same problem as the ``bad faith'' detection. I decided that it was unfeasible to attempt this portion of the project.

\subsection{Brigade detection}
While it is notable that Twitter publicly states that ``the number of reports we receive does not impact whether or not something will be removed'' \cite{twitter-policy}, Twitter does not release any way for non-Twitter-affiliated individuals to be made aware or to discover when people report tweets or accounts. That is, to design a tool that detects when people fraudulently report an account in order to flag it for abuse, you need to know when people are reporting an account. Since this is not something Twitter makes available, there is no way to implement this tool as it stands.

\subsection{Abusive behavior detection outside of swearing}
This proposal involves a catch 22 I did not realize when I submitted it. In order to determine if something is ``abusive behavior'', I need to define what ``abusive behavior'' is. Twitter, of course, is notoriously vague as to what that means, and determining it for myself is non-trivial. If I attempt to use Twitter to determine this (that is, going with my original idea of detecting when accounts were flagged for being abusive and determining if it was just based on recent posts), then the first problem is that the problematic behavior will not be possible to be viewed, as by definition when Twitter flags accounts in this way none of their posts can be seen. This is, of course, ignoring the fact that determining when an account has been flagged is itself non-trivial (it does not appear to be a part of the public API to see when an account is suspended or banned or such). Thus, I had to additionally give up on this project.

\section{Results of implementing tools}
\label{implementation-results}
In my attempt to replicate Botometer \cite{botometer-main}, I used a much smaller sampling of accounts. 64 accounts belonged to human users, and 48 accounts belonged to bots. I also used a much smaller feature set: only focusing on the User, Timing, Friend, and Content features, and at that only focusing on the features that could be collected in one sitting, to replicate the standalone tool. Opposed to the 89\% accuracy found by the study correlated to Botometer \cite{botometer-paper}, I obtained an average accuracy of 80\% (and more specifically, a precision of 79.5\% and a recall of 78.6\%) when using 10-fold cross-validation; considering my extremely small dataset and limited feature set, I am very pleased that my replication was able to achieve so close to the very high figure of the original paper. I also wish that the original paper explained its choice of features more clearly, as it was very nonobvious what their Network features actually measured, and no explanation or code was available pretty much anywhere (humorously, there is a github repo associated with the project, that only includes code to query their own API, behind which they hide their actual implementation).

\section{Conclusion}
\label{conclusion}
Although there is good reason to be interested in the state of Twitter and abusive behavior/bot abuse, particularly regarding more recent studies observing the very high rates of bot participation on the network \cite{bots-twittersphere}, Twitter has in the past six months especially taken drastic steps to curb bot abuse and abusive behavior, to the point that finding abusive behavior in the wild in particular is actually noticeable difficult. It seems that there is some ways to go for Twitter, in particular regarding bot detection (most of the bots identified by these external services are still happily plugging away on Twitter, although some are beginning to go missing), but steps are being made in the right direction. It is a pleasant surprise to see that a social media site is making successful bona fide steps to clean its platform up.

\bibliography{report-bib}
\bibliographystyle{IEEEtran}
\end{document}